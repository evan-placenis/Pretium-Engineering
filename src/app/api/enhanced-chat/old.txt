
-----------------------------------------
import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import { createServerSupabaseClient } from '@/lib/supabase';
import { ReportImage } from '@/lib/supabase';

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Initialize Supabase client
const supabase = createServerSupabaseClient();

// Rough token estimation (1 token ≈ 4 characters for English text)
function estimateTokens(text: string): number {
  return Math.ceil(text.length / 4);
}

// Estimate tokens for a message object
function estimateMessageTokens(message: any): number {
  if (typeof message.content === 'string') {
    return estimateTokens(message.content);
  } else if (Array.isArray(message.content)) {
    return message.content.reduce((total: number, item: any) => {
      if (item.type === 'text') {
        return total + estimateTokens(item.text);
      } else if (item.type === 'image_url') {
        // Images consume significant tokens, estimate conservatively
        return total + 1000; // Rough estimate for image processing
      }
      return total;
    }, 0);
  }
  return 0;
}

// Limit conversation history to stay within token limits
function limitConversationHistory(history: any[], maxTokens: number = 15000): any[] {
  let totalTokens = 0;
  const limitedHistory = [];
  
  // Start from the most recent messages and work backwards
  for (let i = history.length - 1; i >= 0; i--) {
    const message = history[i];
    const messageTokens = estimateMessageTokens(message);
    
    if (totalTokens + messageTokens <= maxTokens) {
      limitedHistory.unshift(message);
      totalTokens += messageTokens;
    } else {
      break;
    }
  }
  
  console.log('Conversation history limited:', {
    original: history.length,
    limited: limitedHistory.length,
    estimatedTokens: totalTokens
  });
  
  return limitedHistory;
}

// Limit images to prevent token overflow
function limitImages(images: any[], maxImages: number = 10): any[] {
  if (images.length <= maxImages) {
    return images;
  }
  
  console.log('Images limited:', {
    original: images.length,
    limited: maxImages
  });
  
  return images.slice(0, maxImages);
}

// Helper function to search embeddings
async function searchProjectEmbeddings(projectId: string, query: string, limit: number = 5) {
  try {
    console.log(`Searching embeddings for project ${projectId} with query: "${query}"`);
    
    if (!process.env.OPENAI_API_KEY) {
      console.error('OpenAI API key not configured');
      return [];
    }
    
    // Generate embedding for the query
    const embeddingResponse = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: query,
    });

    const queryEmbedding = embeddingResponse.data[0].embedding;
    console.log('Generated embedding for query');

    // Search in database using cosine similarity
    const { data, error } = await supabase.rpc('search_embeddings', {
      query_embedding: queryEmbedding,
      project_id: projectId,
      match_threshold: 0.3, // Lowered threshold to find more relevant content
      match_count: limit
    });

    if (error) {
      console.error('Database search error:', error);
      return [];
    }

    const results = data || [];
    console.log(`Found ${results.length} similar chunks for query: "${query}"`);

    // Get additional metadata for results
    const enhancedResults = await Promise.all(
      results.map(async (result: any) => {
        try {
          // Get knowledge document info
          const { data: knowledgeData } = await supabase
            .from('project_knowledge')
            .select('file_name')
            .eq('id', result.knowledge_id)
            .single();

          return {
            content: result.content_chunk,
            similarity: result.similarity,
            chunkIndex: result.chunk_index,
            knowledgeId: result.knowledge_id,
            fileName: knowledgeData?.file_name || 'Unknown file'
          };
        } catch (error) {
          console.error('Error fetching knowledge metadata:', error);
          return {
            content: result.content_chunk,
            similarity: result.similarity,
            chunkIndex: result.chunk_index,
            knowledgeId: result.knowledge_id,
            fileName: 'Unknown file'
          };
        }
      })
    );

    return enhancedResults;
  } catch (error) {
    console.error('Error searching embeddings:', error);
    // Return empty array instead of throwing to prevent 500 errors
    return [];
  }
}

export async function POST(req: NextRequest) {
  try {
    const { 
      reportId, 
      message, 
      reportContent, 
      projectName, 
      bulletPoints,
      images,
      projectId,
      searchKnowledgeBase = false,
      searchQuery = null,
      conversationHistory = [],
      isInitialLoad = false
    } = await req.json();

    console.log('Received enhanced chat request:', { 
      reportId, 
      message, 
      projectName, 
      imagesCount: images?.length || 0,
      searchKnowledgeBase,
      searchQuery,
      isInitialLoad,
      conversationLength: conversationHistory.length
    });

    // Validate required environment variables
    if (!process.env.OPENAI_API_KEY) {
      console.error('OpenAI API key not configured');
      return NextResponse.json(
        { error: 'OpenAI API key not configured' },
        { status: 500 }
      );
    }

    // If this is a knowledge base search request
    if (searchKnowledgeBase && searchQuery && projectId) {
      const results = await searchProjectEmbeddings(projectId, searchQuery, 5);
      return NextResponse.json({
        type: 'search_results',
        results: results,
        query: searchQuery,
        totalFound: results.length
      });
    }

    if (!message) {
      return NextResponse.json(
        { error: 'Message is required' },
        { status: 400 }
      );
    }

    if (!reportContent) {
      return NextResponse.json(
        { error: 'Report content is required' },
        { status: 400 }
      );
    }

    // Validate conversation history format
    if (conversationHistory && !Array.isArray(conversationHistory)) {
      console.error('Invalid conversation history format:', conversationHistory);
      return NextResponse.json(
        { error: 'Invalid conversation history format' },
        { status: 400 }
      );
    }

    // Use images passed in the request (from the current edit session)
    let imagesToUse: ReportImage[] = [];
    
    if (images && images.length > 0) {
      console.log('✅ Images provided for visual analysis:', images.length, 'images');
      console.log('Image URLs:', images.map((img: any) => img.url));
      console.log('Image descriptions:', images.map((img: any) => img.description));
      imagesToUse = images;
    } else {
      console.log('⚡ Fast mode: No images sent (speeds up response)');
    }

        // Create the system prompt (only on initial load)
    const systemPrompt = `
    #ROLE AND CONTEXT:
    You are a senior engineering report assistant at Pretium, a professional engineering consulting firm. 
    You are helping the user modify and improve their existing engineering report for the project: ${projectName || 'a project'}.


    #INSTRUCTIONS:
    - You are an expert assistant helping to modify, improve, or answer questions about this engineering report
    - When the user asks for changes to the report, you have three options:
      OPTION A: If the user asks for a major change (e.g., "reformat the report to be more professional"), provide the COMPLETE updated report in "fullUpdatedContent"
      OPTION B: If the user asks for a simple text change (e.g., "reword section 1.1 to include X and Y"), provide just the updated section in "partialUpdatedContent"
      OPTION C: If the user asks to remove content (e.g., "remove the first image"), analyze the content first to find the exact sections to remove, then use "removeContent"
    - Each bullet that is for an image must independently reference the image using the placeholder format [IMAGE:<image_number>:<GROUP_NAME>].
    - ALWAYS analyze the current report content before making removal decisions - do not guess section names
    - REMEMBER: Images are displayed in the report only when there are bullet points that reference them. Removing all bullet points for an image effectively removes the image from the rendered report.
    - IMPORTANT: If relevant project knowledge is provided in the context, use it to answer questions accurately and provide factual information about specifications, codes, and requirements
    
    #CRITICAL: YOU MUST RESPOND IN VALID JSON FORMAT
    Your response must be a valid JSON object with exactly this structure:
    {
      "message": "Your conversational response explaining what you did or answering their question",
      "fullUpdatedContent": "The complete updated report content (for major changes, otherwise null)",
      "partialUpdatedContent": ["1.1 The updated content", "2.1 The updated content", "GENERAL: The updated content"] or null,
      "removeContent": ["1.1", "2.1", "GENERAL"] or null,
      "embeddingResults": null
    }
    
    #FIELD DEFINITIONS:
    - "message": A SHORT conversational response (1-3 sentences) that explains what you did or answers the user's question. This appears in the chat window.
    - "fullUpdatedContent": The COMPLETE updated report content (for major changes like adding sections, reorganizing, etc.)
    - "partialUpdatedContent": Array of section updates (e.g., ["1.1 The updated content", "2.1 The updated content", "GENERAL: The updated content"]) - each item should start with the section name/header
    - "removeContent": Array of section names to remove (e.g., ["1.1", "2.1", "GENERAL"]) - use this when user asks to remove specific sections or content
    - "embeddingResults": Usually null, unless you found relevant project knowledge to share.
    
    #JSON RULES:
    - ALWAYS start your response with { and end with }
    - Use double quotes around all property names and string values
    - Escape any quotes within strings with backslash
    - Do not include any text before or after the JSON object
    - The "message" field should be SHORT and conversational
    - The "updatedContent" field should contain the ENTIRE report with changes
    - NEVER put the full report content in the "message" field
    
    #INITIALIZATION:
    - When the user says "Initialize chat assistant", provide a friendly welcome message
    - Set "updatedContent" to null for initialization messages
    - Set "message" to a helpful greeting explaining your capabilities


    #WHEN TO UPDATE THE REPORT:
    - Set "fullUpdatedContent" to null when you are only answering questions or providing information without making changes
    - Set "fullUpdatedContent" to the FULL report content when the user requests major modifications, including:
      * Adding new sections
      * Reorganizing content
      * Complete rewrites
      * Removing multiple sections or images
    - Set "partialUpdatedContent" for simple text changes within existing sections
    - Set "removeContent" when the user asks to remove specific sections, images, or content blocks
    
    #IMPORTANT FOR REMOVAL OPERATIONS:
    - When asked to remove images or content, FIRST analyze the current report content to identify:
      * Which sections contain image references [IMAGE:X:Y]
      * Which sections contain the text associated with those images
      * The exact section names/numbers that need to be removed
    - Look for patterns like "1.1", "1.2", "2.1", etc. or named sections like "GENERAL:", "OBSERVATIONS:", etc.
    - Only remove sections that actually contain the requested content
    - If you cannot find the exact section, use "fullUpdatedContent" to rewrite the entire report without the requested content
    - IMPORTANT: When you remove all bullet points that reference an image (e.g., all lines containing [IMAGE:1:ROOF]), the image will automatically be removed from the rendered report since there are no bullet points to display it
    
    #EXAMPLES:
    - User asks: "What does this section mean?" → 
      {
        "message": "This section describes the current condition of the roof structure and identifies areas that need attention.",
        "fullUpdatedContent": null,
        "partialUpdatedContent": null,
        "removeContent": null,
        "embeddingResults": null
      }
    
    - User asks: "Reword the introduction" → 
      {
        "message": "I've reworded the introduction to be more professional and detailed.",
        "fullUpdatedContent": null,
        "partialUpdatedContent": ["GENERAL: The reworded introduction content"],
        "removeContent": null,
        "embeddingResults": null
      }
    
    - User asks: "Change 'good' to 'excellent' in sections 1.1 and 1.2" → 
      {
        "message": "I've updated the word 'good' to 'excellent' in sections 1.1 and 1.2.",
        "fullUpdatedContent": null,
        "partialUpdatedContent": ["1.1 The updated content with 'excellent'", "1.2 The updated content with 'excellent'"],
        "removeContent": null,
        "embeddingResults": null
      }
    
    - User asks: "Remove the first image and its text" → 
      {
        "message": "I've removed the first image and its associated text from the report.",
        "fullUpdatedContent": null,
        "partialUpdatedContent": null,
        "removeContent": ["1.1, 1.2, 1.3"],
        "embeddingResults": null
      }
      Note: The AI should analyze the content to find which section actually contains the first image reference [IMAGE:1:...]. When all bullet points referencing an image are removed, the image will not be displayed in the rendered report.
    
    - User asks: "Remove sections 2.1 and 2.2" → 
      {
        "message": "I've removed sections 2.1 and 2.2 from the report.",
        "fullUpdatedContent": null,
        "partialUpdatedContent": null,
        "removeContent": ["2.1", "2.2"],
        "embeddingResults": null
      }
    
    - User asks: "Add a safety section" → 
      {
        "message": "I've added a new safety recommendations section to your report.",
        "fullUpdatedContent": "GENERAL:\n[existing content]\n\nSAFETY RECOMMENDATIONS:\nBased on the site inspection, the following safety measures should be implemented...\n[rest of complete report with all sections]",
        "partialUpdatedContent": null,
        "removeContent": null,
        "embeddingResults": null
      }

    `;

    // Prepare messages for OpenAI
    const messages: any[] = [];

    // Add system message only on initial load
    if (isInitialLoad) {
      messages.push({
        role: 'system',
        content: systemPrompt
      });
      console.log("Agent initialized!!!!")
    }

    // Add conversation history with token limiting
    if (conversationHistory.length > 0) {
      // Filter out messages with null or empty content
      const validHistory = conversationHistory.filter((msg: any) => 
        msg.content && typeof msg.content === 'string' && msg.content.trim().length > 0
      );
      
      // Limit conversation history to prevent token overflow
      const limitedHistory = limitConversationHistory(validHistory, 12000); // Reserve tokens for current message
      
      console.log('Conversation history processing:', {
        original: conversationHistory.length,
        filtered: validHistory.length,
        limited: limitedHistory.length,
        removed: validHistory.length - limitedHistory.length
      });
      
      messages.push(...limitedHistory); //chat sent to agent
    }

    // For non-initial messages, search for relevant knowledge and add it to the user message
    let userMessageText = message;
    if (!isInitialLoad && projectId) {
      try {
        console.log('Searching embeddings for message:', message);
        const embeddingResults = await searchProjectEmbeddings(projectId, message, 3);
        console.log('Found embedding results:', embeddingResults.length);
        
        if (embeddingResults.length > 0) {
          const relevantKnowledge = '\n\nRELEVANT PROJECT KNOWLEDGE:\n' + 
            embeddingResults.map((result: any, index: number) => 
              `${index + 1}. ${result.content}`
            ).join('\n\n');
          
          userMessageText = `Context: ${relevantKnowledge}\n\nUser question: ${message}`;
          console.log('Enhanced user message with knowledge base content');
        }
      } catch (error) {
        console.error('Error searching embeddings for chat:', error);
        // Continue with original message if embedding search fails
        userMessageText = message;
      }
    }

        // Add current user message with full report context
    const userMessageContent: any[] = [
      { 
        type: 'text', 
        text: `#CURRENT REPORT CONTENT:
${reportContent}

#USER REQUEST:
${userMessageText}

  ${!isInitialLoad ? '\n\nIMPORTANT: You must respond with a valid JSON object in this exact format:\n{\n  "message": "Your short conversational response to user here.",\n  "fullUpdatedContent": "The complete updated report content (for major changes, otherwise null)",\n  "partialUpdatedContent": ["1.1 The updated content", "2.1 The updated content", "GENERAL: The updated content"] or null,\n  "removeContent": ["1.1", "2.1", "GENERAL"] or null,\n  "embeddingResults": null\n}\n\nCRITICAL: Keep the "message" field SHORT and conversational. Use "removeContent" when user asks to remove sections/images. Use "partialUpdatedContent" for simple text changes. Use "fullUpdatedContent" for major rewrites. Each bullet that is for an image must independently reference the image using the placeholder format [IMAGE:<image_number>:<GROUP_NAME>].\n\nFOR REMOVAL OPERATIONS: Analyze the current report content to find the exact sections containing the requested content. Look for image references [IMAGE:X:Y] and identify which sections contain them. Only remove sections that actually exist and contain the requested content. Remember: removing all bullet points that reference an image will effectively remove that image from the rendered report. \n IMPORTANT: If relevant project knowledge is provided in the context, use it to answer questions accurately and provide factual information about specifications, codes, and requirements' : ''}`
      }
    ];

    // Validate that user message content is not null or empty
    if (!userMessageContent[0].text || userMessageContent[0].text.trim().length === 0) {
      console.error('User message content is null or empty:', userMessageContent[0].text);
      return NextResponse.json(
        { error: 'User message content cannot be null or empty' },
        { status: 400 }
      );
    }

    // IMAGE PROCESSING TEMPORARILY DISABLED - Images consume too many tokens
    // Add images if available (with limiting)
    /*
    if (imagesToUse.length > 0) {
      // Limit images to prevent token overflow
      const limitedImages = limitImages(imagesToUse, 8); // Limit to 8 images max
      
      limitedImages.forEach((img, index) => {
        userMessageContent.push(
          {
            type: 'text',
            text: `\n\nCurrent Photo ${index + 1} (${img.tag?.toUpperCase() || 'OVERVIEW'}): ${img.description || 'No description provided'}`
          },
          {
            type: 'image_url',
            image_url: {
              url: img.url,
              detail: 'auto',
            },
          }
        );
      });
    }
    */

    messages.push({
      role: 'user',
      content: userMessageContent
    });

    // Estimate total tokens for the request
    const totalEstimatedTokens = messages.reduce((total, msg) => total + estimateMessageTokens(msg), 0);
    console.log('Sending enhanced request to OpenAI with', messages.length, 'messages, estimated tokens:', totalEstimatedTokens);

    // Check if we're approaching token limits
    if (totalEstimatedTokens > 25000) {
      console.warn('Warning: Request approaching token limit:', totalEstimatedTokens);
    }

    // Call OpenAI API
    let response;
    try {
      response = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: messages,
        temperature: 0.7,
        response_format: { type: "json_object" }
      });
    } catch (openaiError: any) {
      console.error('OpenAI API error:', openaiError);
      
      // Handle specific OpenAI errors
      if (openaiError.code === 'insufficient_quota') {
        return NextResponse.json(
          { error: 'OpenAI quota exceeded. Please try again later.' },
          { status: 429 }
        );
      } else if (openaiError.code === 'invalid_api_key') {
        return NextResponse.json(
          { error: 'OpenAI API key is invalid' },
          { status: 500 }
        );
      } else if (openaiError.status === 429) {
        // Rate limit error - provide more helpful information
        const retryAfter = openaiError.headers?.['retry-after'] || '60';
        const waitTime = parseInt(retryAfter) || 60;
        
        // Check if it's a token limit issue
        if (openaiError.error?.type === 'tokens') {
          return NextResponse.json(
            { 
              error: 'Request too large. Try asking a shorter question or wait a moment before trying again.',
              retryAfter: waitTime,
              type: 'token_limit',
              suggestion: 'Consider breaking your request into smaller parts'
            },
            { status: 429 }
          );
        }
        
        return NextResponse.json(
          { 
            error: `Rate limit reached. Please wait ${waitTime} seconds before trying again.`,
            retryAfter: waitTime,
            type: 'rate_limit'
          },
          { status: 429 }
        );
      } else {
        return NextResponse.json(
          { error: `OpenAI API error: ${openaiError.message || 'Unknown error'}` },
          { status: 500 }
        );
      }
    }

    // Extract and parse the response
    const responseContent = response.choices[0]?.message.content || '';
    console.log('Received enhanced response from OpenAI');
    console.log('Raw response content length:', responseContent.length);
    console.log('Raw response content (first 500 chars):', responseContent.substring(0, 500));
    
    let parsedResponse;
    try {
      parsedResponse = JSON.parse(responseContent);
      console.log('✅ Successfully parsed JSON response');
      console.log('Parsed response:', {
        message: parsedResponse.message ? parsedResponse.message.substring(0, 100) + '...' : 'null',
        hasUpdatedContent: !!parsedResponse.updatedContent,
        updatedContentType: typeof parsedResponse.updatedContent,
        updatedContentLength: parsedResponse.updatedContent ? parsedResponse.updatedContent.length : 0,
        updatedContentPreview: parsedResponse.updatedContent ? parsedResponse.updatedContent.substring(0, 200) + '...' : 'null'
      });
    } catch (error) {
      console.error('❌ Error parsing OpenAI response:', error);
      console.error('Raw response content (full):', responseContent);
      console.error('Response content type:', typeof responseContent);
      console.error('Response content length:', responseContent.length);
      
      // Try to extract JSON from the response if it's wrapped in other text
      let extractedJson = responseContent;
      const jsonMatch = responseContent.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        try {
          extractedJson = jsonMatch[0];
          parsedResponse = JSON.parse(extractedJson);
          console.log('✅ Successfully extracted and parsed JSON from response');
        } catch (extractError) {
          console.error('❌ Failed to extract JSON:', extractError);
        }
      }
      
      if (!parsedResponse) {
        parsedResponse = {
          message: "I had trouble processing your request. Could you try rephrasing?",
          updatedContent: null,
          embeddingResults: null
        };
      }
    }

    return NextResponse.json({
      type: 'chat_response',
      message: parsedResponse.message,
      fullUpdatedContent: parsedResponse.fullUpdatedContent || null,
      partialUpdatedContent: parsedResponse.partialUpdatedContent || null,
      removeContent: parsedResponse.removeContent || null,
      embeddingResults: parsedResponse.embeddingResults || null
    });
  } catch (error: any) {
    console.error('Error in enhanced chat API:', error);
    return NextResponse.json(
      { error: error.message || 'Failed to process enhanced chat request' },
      { status: 500 }
    );
  }
} 